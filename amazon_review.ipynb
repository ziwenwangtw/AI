{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:.conda-pytorch13] *",
      "language": "python",
      "name": "conda-env-.conda-pytorch13-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "amazon_review.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDsPl5ysDBZV",
        "colab_type": "text"
      },
      "source": [
        "### 載入預設的庫(20200507MUST)\n",
        "* 處理bz2\n",
        "* 處理kv值\n",
        "* 載入正規表示法\n",
        "* 載入英文分詞庫\n",
        "* numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDR7zAOoDLCk",
        "colab_type": "code",
        "outputId": "3bb09788-5198-496a-8047-f0624a10990a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwKoD59ADBZX",
        "colab_type": "code",
        "outputId": "1ab704fb-95bf-4dcc-9c13-a90e3f27bb4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_file = bz2.BZ2File('/content/drive/My Drive/train.ft.txt.bz2')\n",
        "test_file = bz2.BZ2File('/content/drive/My Drive/test.ft.txt.bz2')\n",
        "\n",
        "train_file = train_file.readlines()\n",
        "test_file = test_file.readlines()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohVACmVODBZd",
        "colab_type": "text"
      },
      "source": [
        "### 看一下檔案中有什麼，並且看一共有多大"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76X7nGp6DBZe",
        "colab_type": "code",
        "outputId": "cf188703-a5be-4882-9a58-726636d77dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'__label__2 Even Mommy has fun with this one!: My four year old daughter loves everything Barbie and loves the Rapunzel movie. This game is tons of fun, even for a 42 year old. We love playing it together. We love decorating all the rooms and finding the gems. What even better is, she can play it alone and I get some me time!\\n'\n",
            "3600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-mmt0ODBZi",
        "colab_type": "text"
      },
      "source": [
        "### 原檔案太大，只取其中80萬，原來有360萬\n",
        "### 同時將檔案轉成utf8格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqCVwNWkDBZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
        "num_test = 200000  # Using 200,000 reviews from test set\n",
        "# num_train = len(train_file)\n",
        "# num_test = len(test_file)\n",
        "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
        "test_file = [x.decode('utf-8') for x in test_file[:num_test]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-lSB3JDBZn",
        "colab_type": "text"
      },
      "source": [
        "### 看一下檔案中有什麼，並且看一共有多大"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiU_Xah0DBZn",
        "colab_type": "code",
        "outputId": "1390e67f-a2d3-424e-df49-93e1a98008c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(train_file[67])\n",
        "print(len(train_file))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__label__2 Even Mommy has fun with this one!: My four year old daughter loves everything Barbie and loves the Rapunzel movie. This game is tons of fun, even for a 42 year old. We love playing it together. We love decorating all the rooms and finding the gems. What even better is, she can play it alone and I get some me time!\n",
            "\n",
            "800000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEijn9ujDBZr",
        "colab_type": "text"
      },
      "source": [
        "### 將標記抓出，正評為1、負評為0，同時對訓練集和測試集進行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZecW9lLDBZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQyVEcr1DBZv",
        "colab_type": "text"
      },
      "source": [
        "### 看一下標記和資料的長相"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF2zpqXFDBZw",
        "colab_type": "code",
        "outputId": "f706efbf-4595-4601-e47d-827c7d312acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(len(train_labels))\n",
        "print(train_labels[67])\n",
        "print(train_sentences[67])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800000\n",
            "1\n",
            "even mommy has fun with this one!: my four year old daughter loves everything barbie and loves the rapunzel movie. this game is tons of fun, even for a 42 year old. we love playing it together. we love decorating all the rooms and finding the gems. what even better is, she can play it alone and i get some me time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqUQrsjDBZ0",
        "colab_type": "text"
      },
      "source": [
        "### 把數字全部變成0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3N9FFPRDBZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some simple cleaning of data\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P-n7xXYDBZ4",
        "colab_type": "text"
      },
      "source": [
        "### 找一筆資料出來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQjbrT7XDBZ5",
        "colab_type": "code",
        "outputId": "61f70a07-ed06-4f71-d020-1d957c066acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(train_sentences[67])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "even mommy has fun with this one!: my four year old daughter loves everything barbie and loves the rapunzel movie. this game is tons of fun, even for a 00 year old. we love playing it together. we love decorating all the rooms and finding the gems. what even better is, she can play it alone and i get some me time!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7_UsXpiDBZ8",
        "colab_type": "text"
      },
      "source": [
        "### 把網址全部換成'\\<url>'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th_2yny4DBZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modify URLs to <url>\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JesaOQuUDBaA",
        "colab_type": "text"
      },
      "source": [
        "### 找一筆出來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iB4hAjTDBaB",
        "colab_type": "code",
        "outputId": "1939bc93-e130-4cc5-a2ff-c0b561a057c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "with_url = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if '<url>' in s:\n",
        "        with_url = ii\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[5])\n",
        "print(train_sentences[with_url])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "__label__2 an absolute masterpiece: I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda's music contributed greatly to the mood of every single minute of the whole game.Composed of 3 CDs and quite a few songs (I haven't an exact count), all of which are heart-rendering and impressively remarkable, this soundtrack is one I assure you you will not forget. It has everything for every listener -- from fast-paced and energetic (Dancing the Tokage or Termina Home), to slower and more haunting (Dragon God), to purely beautifully composed (Time's Scar), to even some fantastic vocals (Radical Dreamers).This is one of the best videogame soundtracks out there, and surely Mitsuda's best ever. ^_^\n",
            "\n",
            "an absolute masterpiece: i am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. and whether you were aware of it or not, mitsuda's music contributed greatly to the mood of every single minute of the whole <url>posed of 0 cds and quite a few songs (i haven't an exact count), all of which are heart-rendering and impressively remarkable, this soundtrack is one i assure you you will not forget. it has everything for every listener -- from fast-paced and energetic (dancing the tokage or termina home), to slower and more haunting (dragon god), to purely beautifully composed (time's scar), to even some fantastic vocals (radical <url>s is one of the best videogame soundtracks out there, and surely mitsuda's best ever. ^_^\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tdR51EEDBaF",
        "colab_type": "text"
      },
      "source": [
        "### 將句子分詞為單字\n",
        "* 啟用Counter\n",
        "* 先將某條句定清空，再將分詞完之後的單詞存入該句子，使用ntlk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e61rHlugDBaG",
        "colab_type": "code",
        "outputId": "4a5de8a3-f8f4-4590-cd60-b5d7fa8cc4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    # The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
        "        words.update([word.lower()])  # Converting all the words to lowercase\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0% done\n",
            "2.5% done\n",
            "5.0% done\n",
            "7.5% done\n",
            "10.0% done\n",
            "12.5% done\n",
            "15.0% done\n",
            "17.5% done\n",
            "20.0% done\n",
            "22.5% done\n",
            "25.0% done\n",
            "27.5% done\n",
            "30.0% done\n",
            "32.5% done\n",
            "35.0% done\n",
            "37.5% done\n",
            "40.0% done\n",
            "42.5% done\n",
            "45.0% done\n",
            "47.5% done\n",
            "50.0% done\n",
            "52.5% done\n",
            "55.0% done\n",
            "57.5% done\n",
            "60.0% done\n",
            "62.5% done\n",
            "65.0% done\n",
            "67.5% done\n",
            "70.0% done\n",
            "72.5% done\n",
            "75.0% done\n",
            "77.5% done\n",
            "80.0% done\n",
            "82.5% done\n",
            "85.0% done\n",
            "87.5% done\n",
            "90.0% done\n",
            "92.5% done\n",
            "95.0% done\n",
            "97.5% done\n",
            "100% done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTwR_zQoDBaJ",
        "colab_type": "text"
      },
      "source": [
        "### 查看分完詞後的句子長相"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg49mzKCDBaJ",
        "colab_type": "code",
        "outputId": "28c653a0-5484-481d-d653-6f0b8f899d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(train_sentences[67])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['even', 'mommy', 'has', 'fun', 'with', 'this', 'one', '!', ':', 'my', 'four', 'year', 'old', 'daughter', 'loves', 'everything', 'barbie', 'and', 'loves', 'the', 'rapunzel', 'movie', '.', 'this', 'game', 'is', 'tons', 'of', 'fun', ',', 'even', 'for', 'a', '00', 'year', 'old', '.', 'we', 'love', 'playing', 'it', 'together', '.', 'we', 'love', 'decorating', 'all', 'the', 'rooms', 'and', 'finding', 'the', 'gems', '.', 'what', 'even', 'better', 'is', ',', 'she', 'can', 'play', 'it', 'alone', 'and', 'i', 'get', 'some', 'me', 'time', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ5LzLfzDBaN",
        "colab_type": "text"
      },
      "source": [
        "### 把只出現一次的字刪除，只留出現兩次以上的字\n",
        "### 把字照出現次數排序"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Miv1UmDBaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6fde3a1f-6f5b-40ce-96d7-31adcda90a72"
      },
      "source": [
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a68997e500b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Sorting the words according to the number of appearances, with the most common word being first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEHKnIlODBaR",
        "colab_type": "text"
      },
      "source": [
        "### 印出一個字來看長什麼樣子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtVHMqOWDBaR",
        "colab_type": "code",
        "outputId": "8af59f17-76d7-491d-aa5d-1190130583e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(words))\n",
        "print(words[100])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "225962\n",
            "she\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-KU04IJDBaU",
        "colab_type": "text"
      },
      "source": [
        "### 將未知字和補0算一個字加進去\n",
        "* 開始將字編號\n",
        "* 建立字對編號的關係，雙向所以有兩個"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Ked48WDBaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzOhoCnjDBaY",
        "colab_type": "text"
      },
      "source": [
        "### 印出某個字來看其索引，並且印出索引看字是什麼"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp1y1rgkDBaZ",
        "colab_type": "code",
        "outputId": "44120254-d91c-4c01-993a-1ac381c950a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(word2idx['the'])\n",
        "print(idx2word[100])\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "after\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ybZZBJWDBac",
        "colab_type": "text"
      },
      "source": [
        "### 把訓練句子中的所有文字都轉成索引，還有測試句子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Nv4OULDBad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a6d2fe4b-690c-4c51-cba0-1e9454d69df9"
      },
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-52ec5a24269e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# For test sentences, we have to tokenize the sentences as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtest_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dTdUAH2DBaf",
        "colab_type": "text"
      },
      "source": [
        "### 印出一個句子出來看看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTKkIHlDBag",
        "colab_type": "code",
        "outputId": "dc31ccca-57d6-4411-ebe5-632df8ef5fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4DkVaMJDBal",
        "colab_type": "text"
      },
      "source": [
        "### 印一個超過200的句子來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zJgerxDBan",
        "colab_type": "code",
        "outputId": "84bd5a14-efa0-4480-fcee-42fe72bb107b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "more_than_200 = 0\n",
        "for ii, s in enumerate(train_sentences):\n",
        "    if len(s) > 200:\n",
        "        more_than_200 = ii\n",
        "        print(len(s))\n",
        "        print(ii)\n",
        "        break\n",
        "print(train_file[more_than_200])\n",
        "print(train_sentences[more_than_200])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "204\n",
            "71\n",
            "__label__1 barbie rapunzel = crying child: My 6-year old daughter is currently sobbing in her bedroom as a result of this rotten game.She spent an hour on this game painting in pretty pictures and coloring in flowers. But halfway through the game, the program hung, and wouldn't let us leave the room or click anywhere else. Now even if we exit the game and then start it back up, we are frozen in one spot while Barbie's disembodied voice urges us to \"explore somewhere else in the castle.\"Standard software troubleshooting and rebooting didn't help. Vivendi Games' support site is \"currently disabled.\" And now I'm left with a weeping, frustrating child who doesn't understand that it is not her fault that her game won't work. I'm hoping it was just incompetent software programming that couldn't catch the bug my 6-year old caught the first time around. Or maybe they just don't care about the problems, so long as you get our 20 bucks. If you don't want a crying little girl, avoid this game!\n",
            "\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AylMwgbVDBau",
        "colab_type": "text"
      },
      "source": [
        "### 把句子不到200的前面補上0，如果超過200，就把200以後的刪除，讓每個句子的長度都是200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUqamPnODBaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)\n",
        "\n",
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWKhAAEFDBa3",
        "colab_type": "text"
      },
      "source": [
        "### 印一個補過0的出來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh8OYYFpDBa5",
        "colab_type": "code",
        "outputId": "2506702a-e08f-443f-8d23-1c555646ff4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(len(train_sentences[67]))\n",
        "print(train_sentences[67])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C0iMApxDBbB",
        "colab_type": "text"
      },
      "source": [
        "### 印出一個被截斷的來看"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgElmGCRDBbB",
        "colab_type": "code",
        "outputId": "499bcd56-f4a3-461c-852a-27c5c2544b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(len(train_sentences[more_than_200]))\n",
        "print(train_sentences[more_than_200])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdnSzWFzDBbE",
        "colab_type": "text"
      },
      "source": [
        "### 把測試資料集分成test和validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNmVv4U3DBbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.5 # 50% validation, 50% test\n",
        "split_id = int(split_frac * len(test_sentences))\n",
        "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
        "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xguy7v2DBbH",
        "colab_type": "text"
      },
      "source": [
        "### 看測試資料集的長度為多少"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvXDnFNCDBbI",
        "colab_type": "code",
        "outputId": "12ee2002-e880-4f99-9bc8-dc1edcb1d1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(val_sentences))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3OP3wMQDBbK",
        "colab_type": "text"
      },
      "source": [
        "### 將資料載入pytorch格式\n",
        "* 三個資料集的載入\n",
        "* 設定批次大小\n",
        "* 建立dataloader的generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcQx049tDBbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqdDV1eRDBbO",
        "colab_type": "text"
      },
      "source": [
        "### 檢查是否有gpu存在"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lc7wQ40DBbP",
        "colab_type": "code",
        "outputId": "535d6c8f-dfd0-47bf-b95b-9566017ac186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJj3Nk6eDBbS",
        "colab_type": "text"
      },
      "source": [
        "### 建立網路\n",
        "* `vocab_size`：字彙檔的大小，就是一個one-hot的資料\n",
        "* `output_size`：大小為1，因為只要一個值，介於0和1之間\n",
        "* `embedding_dim`：就是word2vec的特徵大小，從`vocab_size`降維過來的\n",
        "* `hidden_dim`：hidden和cell states的維度\n",
        "* `n_layers`：lstm的層數\n",
        "* `drop_prob`：dropout的比例\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0YV-FhZDBbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGN62rAmDBbV",
        "colab_type": "text"
      },
      "source": [
        "### 定義超參數\n",
        "* 定義超參數\n",
        "* 建立模型\n",
        "* 將模型移入gpu中\n",
        "* 定義學習率\n",
        "* 定義損失函數\n",
        "* 定義最佳化方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwbAIpvnDBbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "\n",
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTFhuc8LDBbY",
        "colab_type": "text"
      },
      "source": [
        "## 接下來把整個網路手動走一遍"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNflX6ogDBbY",
        "colab_type": "text"
      },
      "source": [
        "### 啟始化hidden和cell states的值\n",
        "\n",
        "* 由於model已經移入gpu，因此呼叫model的函數產生的值也會在gpu中，就會出現cuda device\n",
        "* 另外要建立tuple，每一個批次都要有自己的tuple，不是一個大tuple丟去就好了"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5jlDmdzDBbZ",
        "colab_type": "code",
        "outputId": "a869244c-685c-49de-9981-ae57f85713ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "hh = model.init_hidden(batch_size)\n",
        "hh = tuple([e.data for e in hh])\n",
        "print(hh[1].shape)\n",
        "print(hh[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 400, 512])\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re3B-iLUDBbb",
        "colab_type": "code",
        "outputId": "070cb372-9f01-430a-d90f-0e84f1dfb96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "inputs = next(iter(train_loader)) # 取一段training data出來\n",
        "x = inputs[0].to(device) #把資料移入gpu\n",
        "batch_num = x.size(0) #取得批次大小，256\n",
        "x = x.long() # 把輸入值轉成長整數\n",
        "print(x.shape) # 看看x的型狀長什麼樣子\n",
        "print(x) #印出一個x來看看"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 200])\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REBuVIRMDBbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, embedding_dim) # 把x從one-hot的詞彙大小轉成word2vec的大小\n",
        "embedding = embedding.to(device) #將這個函數放入gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDU7ANkdDBbh",
        "colab_type": "code",
        "outputId": "8aefa0e3-6f70-44aa-99a8-79487906dbcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeds = embedding(x) # 將詞彙轉成wordvec\n",
        "embeds.shape #看看型狀"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 200, 400])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYoFH8j8DBbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=0.5, batch_first=True) # 建立一個lstm\n",
        "lstm = lstm.to(device) #將lstm移入gpu\n",
        "lstm_out, hidden = lstm(embeds, hh) #讓剛才的資料跑過lstm，產生輸出值及新的hidden值"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2GIXltXDBbn",
        "colab_type": "code",
        "outputId": "9b2fa550-781c-4b87-d37e-c5c30430b014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(lstm_out.shape) #看看從lstm跑出來的東西型狀\n",
        "print(hidden[0].shape) # 看看hidden第一個值的形狀"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 200, 512])\n",
            "torch.Size([2, 400, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWQ_xvihDBbr",
        "colab_type": "code",
        "outputId": "67c08c69-118d-400d-d686-6d99685f0f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lstm_out = lstm_out.contiguous().view(-1, hidden_dim) #把lstm的輸出值攤平\n",
        "lstm_out.shape #看看形狀"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80000, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1_6KXZDDBbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = nn.Dropout(0.5) #設定dropout\n",
        "out = dropout(lstm_out) # 讓輸出值走一次dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAWIfpEmDBbw",
        "colab_type": "code",
        "outputId": "77a5a95e-7721-4efc-c446-ba82a711a980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(out.shape) # 查看輸出值的形狀"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80000, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJbeGmeZDBb0",
        "colab_type": "code",
        "outputId": "dca3fcb1-7c18-4e5b-aff6-ee1a94d5c8f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fc = nn.Linear(hidden_dim, 1) #定義最後一層fc\n",
        "fc = fc.to(device) #移入gpu\n",
        "out = fc(out) #讓輸入出值走一次fc\n",
        "print(out.shape) # 看一下形狀"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mux3eSbtDBb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigmoid = nn.Sigmoid() # 讓輸出值介於0和1之間\n",
        "sigmoid = sigmoid.to(device) #將sigmoid移入gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFOYqVqbDBb6",
        "colab_type": "code",
        "outputId": "790c06f6-496b-4947-922f-4a595531586d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "out = sigmoid(out) #讓輸出值走一次sigmoid\n",
        "print(out.shape) #看看形狀"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80000, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_BBeHjKDBb8",
        "colab_type": "code",
        "outputId": "5e12de6c-fba2-45b2-ce43-7c0d79fe4546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "out = out.view(batch_size, -1) #把輸出值還原成(批次大小，句子長度)\n",
        "print(out.shape) # 看形狀"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDa49GqDBcA",
        "colab_type": "code",
        "outputId": "182f7043-b9dc-4148-ec02-31f7f831553a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "out = out[:,-1] #取出最後結果值\n",
        "print(out.shape)# 看形狀"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([400])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrBsHJ0kDBcC",
        "colab_type": "code",
        "outputId": "523245be-2391-4e79-8125-b24d979b6e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "print(out) # 看每批256個輸出值"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.4930, 0.4908, 0.5033, 0.5100, 0.4888, 0.5135, 0.5229, 0.4968, 0.5411,\n",
            "        0.5125, 0.4620, 0.5028, 0.4951, 0.5034, 0.4854, 0.4997, 0.5095, 0.4993,\n",
            "        0.5106, 0.4859, 0.5087, 0.5064, 0.4991, 0.4910, 0.4825, 0.4966, 0.4965,\n",
            "        0.4981, 0.4886, 0.4912, 0.4772, 0.5114, 0.5116, 0.4838, 0.5051, 0.4979,\n",
            "        0.5131, 0.4852, 0.4915, 0.4826, 0.5022, 0.4861, 0.5062, 0.4833, 0.4814,\n",
            "        0.4954, 0.5064, 0.4952, 0.4889, 0.4992, 0.4995, 0.4947, 0.5171, 0.4964,\n",
            "        0.4968, 0.5013, 0.4892, 0.5368, 0.4945, 0.5102, 0.5102, 0.5069, 0.4959,\n",
            "        0.4932, 0.5072, 0.5042, 0.5003, 0.5065, 0.5032, 0.5037, 0.4938, 0.4824,\n",
            "        0.4991, 0.4945, 0.5026, 0.4811, 0.5099, 0.4908, 0.5042, 0.5024, 0.4934,\n",
            "        0.4732, 0.5180, 0.5099, 0.5017, 0.4783, 0.4758, 0.4947, 0.4923, 0.4916,\n",
            "        0.4898, 0.5088, 0.4972, 0.4892, 0.5084, 0.4830, 0.4961, 0.4856, 0.4820,\n",
            "        0.4855, 0.4842, 0.4861, 0.5129, 0.5133, 0.4934, 0.5235, 0.5014, 0.4933,\n",
            "        0.4868, 0.5028, 0.4927, 0.5015, 0.4628, 0.4886, 0.5109, 0.5049, 0.5034,\n",
            "        0.4964, 0.4960, 0.5053, 0.5105, 0.5129, 0.4990, 0.5117, 0.4976, 0.5115,\n",
            "        0.4987, 0.5062, 0.5060, 0.5000, 0.4911, 0.4943, 0.5122, 0.4930, 0.4833,\n",
            "        0.4950, 0.5035, 0.4809, 0.5079, 0.5158, 0.5036, 0.5103, 0.5046, 0.5138,\n",
            "        0.5078, 0.5010, 0.4799, 0.5013, 0.5255, 0.5040, 0.5078, 0.4656, 0.5090,\n",
            "        0.5119, 0.5120, 0.5044, 0.5205, 0.4922, 0.5072, 0.5012, 0.4948, 0.4941,\n",
            "        0.4980, 0.5222, 0.5012, 0.5033, 0.4954, 0.5164, 0.5129, 0.5028, 0.4941,\n",
            "        0.4798, 0.5175, 0.5148, 0.5063, 0.4800, 0.4972, 0.4931, 0.4793, 0.4834,\n",
            "        0.4963, 0.5058, 0.4924, 0.4878, 0.5335, 0.4871, 0.5048, 0.5065, 0.5273,\n",
            "        0.4904, 0.5097, 0.4949, 0.5067, 0.4823, 0.5153, 0.5112, 0.5140, 0.4981,\n",
            "        0.4764, 0.4862, 0.5032, 0.5131, 0.5108, 0.4797, 0.5156, 0.5009, 0.4795,\n",
            "        0.5041, 0.5093, 0.4990, 0.5020, 0.4893, 0.4777, 0.4892, 0.5007, 0.4985,\n",
            "        0.4990, 0.5062, 0.4977, 0.5012, 0.4701, 0.4839, 0.5025, 0.5039, 0.5053,\n",
            "        0.5075, 0.4927, 0.5094, 0.5181, 0.5090, 0.4966, 0.5217, 0.5007, 0.5006,\n",
            "        0.4956, 0.4996, 0.4941, 0.4921, 0.4816, 0.4959, 0.5018, 0.5016, 0.4904,\n",
            "        0.4892, 0.5045, 0.5021, 0.5099, 0.5049, 0.4908, 0.5196, 0.4872, 0.4792,\n",
            "        0.4979, 0.4936, 0.4917, 0.4816, 0.5000, 0.4922, 0.4965, 0.5026, 0.5007,\n",
            "        0.5087, 0.5021, 0.5038, 0.5116, 0.4883, 0.4924, 0.4897, 0.4914, 0.5021,\n",
            "        0.4960, 0.5235, 0.4886, 0.4910, 0.4932, 0.4813, 0.5009, 0.5007, 0.4999,\n",
            "        0.4971, 0.5075, 0.4948, 0.4685, 0.5267, 0.4782, 0.5121, 0.5026, 0.4996,\n",
            "        0.5000, 0.5123, 0.5068, 0.5011, 0.4986, 0.5078, 0.4940, 0.5005, 0.4999,\n",
            "        0.4954, 0.5081, 0.5045, 0.5044, 0.5088, 0.4964, 0.5017, 0.4850, 0.4946,\n",
            "        0.4963, 0.5025, 0.4808, 0.5105, 0.5072, 0.4875, 0.5018, 0.5038, 0.5240,\n",
            "        0.4892, 0.5109, 0.5111, 0.5067, 0.4954, 0.5199, 0.4962, 0.5094, 0.4935,\n",
            "        0.5085, 0.5101, 0.5112, 0.4970, 0.4885, 0.5028, 0.5046, 0.4782, 0.4846,\n",
            "        0.4915, 0.5022, 0.4963, 0.5057, 0.4789, 0.5127, 0.5106, 0.5047, 0.5037,\n",
            "        0.4734, 0.4883, 0.4862, 0.4927, 0.4841, 0.5014, 0.5064, 0.5041, 0.4893,\n",
            "        0.5058, 0.5033, 0.5148, 0.4871, 0.5087, 0.4901, 0.5251, 0.5133, 0.5130,\n",
            "        0.4896, 0.4940, 0.4810, 0.5052, 0.5007, 0.4879, 0.4854, 0.5099, 0.5046,\n",
            "        0.4631, 0.4791, 0.5204, 0.5122, 0.5122, 0.4999, 0.5103, 0.5049, 0.5067,\n",
            "        0.4961, 0.5001, 0.4970, 0.5099, 0.5046, 0.4939, 0.4904, 0.5305, 0.5031,\n",
            "        0.5169, 0.5173, 0.5140, 0.4978, 0.5048, 0.5064, 0.4880, 0.4825, 0.4879,\n",
            "        0.5105, 0.5084, 0.4960, 0.5079], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28DRQK8nDBcF",
        "colab_type": "text"
      },
      "source": [
        "### 接下來進行真正訓練\n",
        "* 輪數為2\n",
        "* 計數值\n",
        "* 每1000步就印出結果\n",
        "* 一開始的loss值為無限大\n",
        "\n",
        "### 開始訓練\n",
        "* 初始化hidden值\n",
        "* 走訓練流程\n",
        "* 每1000步就印出結果一次"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ISJOsjDBcF",
        "colab_type": "code",
        "outputId": "f8e218fb-b631-4166-cd51-279df51821e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2... Step: 1000... Loss: 0.692882... Val Loss: 0.693246\n",
            "Validation loss decreased (inf --> 0.693246).  Saving model ...\n",
            "Epoch: 1/2... Step: 2000... Loss: 0.692886... Val Loss: 0.693209\n",
            "Validation loss decreased (0.693246 --> 0.693209).  Saving model ...\n",
            "Epoch: 2/2... Step: 3000... Loss: 0.693592... Val Loss: 0.693362\n",
            "Epoch: 2/2... Step: 4000... Loss: 0.694045... Val Loss: 0.693282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtBcJ24lDBcH",
        "colab_type": "text"
      },
      "source": [
        "### 進行模型結果預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59jo-DcsDBcI",
        "colab_type": "code",
        "outputId": "15703d72-07e0-4310-c10d-1d4e54fdaff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))\n",
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.693\n",
            "Test accuracy: 50.334%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}